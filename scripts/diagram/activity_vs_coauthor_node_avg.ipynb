{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "import pyspark.sql.functions as sparkf\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "coauthor_dir = \"s3://recsys-bucket-1/data_lake/arnet/tables/coauthor/merge-0\"\n",
    "author_activ_dir = \"s3://recsys-bucket-1/data_lake/arnet/tables/author_activity/merge-0\"\n",
    "\n",
    "coauthor_schema = StructType([\n",
    "    StructField('_id', StringType(), False),\n",
    "    StructField('_status', IntegerType(), False),\n",
    "    StructField('_order', IntegerType(), False),\n",
    "    StructField('paper_id', StringType(), False),\n",
    "    StructField('paper_title', StringType(), False),\n",
    "    StructField('author1_id', StringType(), False),\n",
    "    StructField('author1_name', StringType(), False),\n",
    "    StructField('author1_org', StringType(), False),\n",
    "    StructField('author2_id', StringType(), False),\n",
    "    StructField('author2_name', StringType(), False),\n",
    "    StructField('author2_org', StringType(), False),\n",
    "    StructField('year', FloatType(), False),\n",
    "])\n",
    "\n",
    "author_activ_schema = StructType([\n",
    "    StructField('author_id', StringType(), False),\n",
    "    StructField('freq', FloatType(), False),\n",
    "])\n",
    "\n",
    "coauthor_df = spark.read.schema(coauthor_schema).parquet(coauthor_dir)\n",
    "\n",
    "author_activ_df = spark.read.schema(author_activ_schema).parquet(author_activ_dir)\n",
    "author_activ_df.createOrReplaceTempView(\"author_activ_df\")\n",
    "\n",
    "sample_coauthor = coauthor_df.sample(0.1, 999)\n",
    "sample_coauthor.createOrReplaceTempView(\"sample_coauth\")\n",
    "\n",
    "join_freq_df = spark.sql(\"\"\"\n",
    "    select sc.author1_id, sc.author2_id, aad1.freq as freq1, aad2.freq as freq2\n",
    "    from sample_coauth as sc\n",
    "        inner join author_activ_df as aad1 on aad1.author_id = sc.author1_id\n",
    "        inner join author_activ_df as aad2 on aad2.author_id = sc.author2_id\n",
    "\"\"\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "def avg_freq_func(v1: pd.Series, v2: pd.Series) -> pd.Series:\n",
    "    v1_log = np.log(v1 + 10.)\n",
    "    v2_log = np.log(v2 + 10.)\n",
    "    return (v1_log + v2_log).abs() / 2\n",
    "\n",
    "avg_freq = pandas_udf(avg_freq_func, returnType=FloatType())\n",
    "\n",
    "sample_plus = join_freq_df.select(avg_freq(sparkf.col(\"freq1\"), sparkf.col(\"freq2\")).alias(\"node_plus\"))\n",
    "sample_plus.createOrReplaceTempView(\"sample_plus_df\")\n",
    "\n",
    "max_val = spark.sql(\"select max(node_plus) as max_val from sample_plus_df\").collect()[0]['max_val']\n",
    "min_val = spark.sql(\"select min(node_plus) as min_val from sample_plus_df\").collect()[0]['min_val']\n",
    "\n",
    "res_df = sample_plus.limit(5000).repartition(1) \\\n",
    "    .withColumn(\"node_plus\", (sparkf.col(\"node_plus\") - min_val) / (max_val - min_val)) \\\n",
    "    .withColumn(\"datapoint\", sparkf.monotonically_increasing_id()) \\\n",
    "    .select(sparkf.col(\"datapoint\"), sparkf.col(\"node_plus\")) \\\n",
    "\n",
    "res_df.repartition(1).write.mode(\"overwrite\").csv(\"s3://recsys-bucket-1/diagram/activity_vs_coauthor_node_avg\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
