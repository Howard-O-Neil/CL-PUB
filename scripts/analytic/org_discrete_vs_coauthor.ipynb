{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "import sys\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# each JSON is small, there's no need in iterative processing\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import xml\n",
    "import time\n",
    "\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, FloatType, ArrayType\n",
    "import pyspark.sql.functions as sparkf\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "import copy\n",
    "import uuid\n",
    "\n",
    "spark = (pyspark.sql.SparkSession.builder.getOrCreate())\n",
    "\n",
    "coauthor_dir = \"gs://clpub/data_lake/arnet/tables/coauthor/merge-0\"\n",
    "author_org_group_dir = \"gs://clpub/data_lake/arnet/tables/author_org_discrete/merge-0\"\n",
    "\n",
    "from typing import Iterator, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "coauthor_schema = StructType([\n",
    "    StructField('_id', StringType(), False),\n",
    "    StructField('_status', IntegerType(), False),\n",
    "    StructField('_order', IntegerType(), False),\n",
    "    StructField('paper_id', StringType(), False),\n",
    "    StructField('paper_title', StringType(), False),\n",
    "    StructField('author1_id', StringType(), False),\n",
    "    StructField('author1_name', StringType(), False),\n",
    "    StructField('author1_org', StringType(), False),\n",
    "    StructField('author2_id', StringType(), False),\n",
    "    StructField('author2_name', StringType(), False),\n",
    "    StructField('author2_org', StringType(), False),\n",
    "    StructField('year', FloatType(), False),\n",
    "])\n",
    "coauthor_df = spark.read.schema(coauthor_schema).parquet(coauthor_dir)\n",
    "coauthor_df.createOrReplaceTempView(\"coauthor_df\")\n",
    "\n",
    "author_org_group_schema = StructType([\n",
    "    StructField('author_id', StringType(), False),\n",
    "    StructField(\"author_org\", StringType(), False),\n",
    "    StructField('org_rank', FloatType(), False),\n",
    "    StructField('computed', IntegerType(), False),\n",
    "])\n",
    "author_org_group_df = spark.read.schema(author_org_group_schema).parquet(author_org_group_dir)\n",
    "author_org_group_df.createOrReplaceTempView(\"author_org_group_df\")\n",
    "\n",
    "group_coauthor = spark.sql(\"\"\"\n",
    "    select author1_id, author2_id, author1_org, author2_org, count(_id) as collab\n",
    "    from coauthor_df\n",
    "    group by author1_id, author2_id, author1_org, author2_org\n",
    "\"\"\")\n",
    "\n",
    "sample = group_coauthor.sample(0.1, 999)\n",
    "sample.createOrReplaceTempView(\"coauthor_sample\")\n",
    "\n",
    "sample_ranking = spark.sql(\"\"\"\n",
    "    select cs.collab, aogd1.org_rank as author1_rank, aogd2.org_rank as author2_rank\n",
    "    from coauthor_sample as cs\n",
    "        inner join author_org_group_df as aogd1 on aogd1.author_id = cs.author1_id and aogd1.author_org = cs.author1_org\n",
    "        inner join author_org_group_df as aogd2 on aogd2.author_id = cs.author2_id and aogd2.author_org = cs.author2_org\n",
    "    limit 5000\n",
    "\"\"\")\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "@pandas_udf(\"float\", PandasUDFType.SCALAR)\n",
    "def node_proximity(v1, v2):\n",
    "    list_r1     =  v1.values.tolist()\n",
    "    list_r2     =  v2.values.tolist() \\\n",
    "    \n",
    "    list_res    = []\n",
    "    for idx in range(0, len(list_r1)):\n",
    "        proximity = 1.\n",
    "        if list_r1[idx] > 0 and list_r2[idx] > 0:\n",
    "            proximity = abs(list_r1[idx] - list_r2[idx]) \\\n",
    "                / max(list_r1[idx], list_r2[idx])\n",
    "        list_res.append(proximity) \\\n",
    "    \n",
    "    return pd.Series(list_res)\n",
    "\n",
    "sample_proximity = sample_ranking.repartition(1).withColumn(\"datapoint\", sparkf.monotonically_increasing_id()) \\\n",
    "    .select(sparkf.col(\"datapoint\"), sparkf.col(\"collab\"), sparkf.col(\"author1_rank\"), sparkf.col(\"author2_rank\"), \\\n",
    "    node_proximity(sparkf.col(\"author1_rank\"), sparkf.col(\"author2_rank\")).alias(\"node_proximity\"))\n",
    "\n",
    "sample_proximity.repartition(1).write.mode(\"overwrite\").csv(\"gs://clpub/diagram/org_discrete_vs_coauthor\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
