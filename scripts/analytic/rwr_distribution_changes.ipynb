{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "import sys\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# each JSON is small, there's no need in iterative processing\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import xml\n",
    "import time\n",
    "\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, FloatType, ArrayType\n",
    "import pyspark.sql.functions as sparkf\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "import copy\n",
    "import uuid\n",
    "\n",
    "spark = (pyspark.sql.SparkSession.builder.getOrCreate())\n",
    "\n",
    "citation_rwr_iter1_dir       = \"gs://clpub/data_lake/arnet/tables/citation_rwr/iter-0\"\n",
    "citation_rwr_iter2_dir       = \"gs://clpub/data_lake/arnet/tables/citation_rwr/iter-1\"\n",
    "citation_rwr_iter5_dir       = \"gs://clpub/data_lake/arnet/tables/citation_rwr/iter-4\"\n",
    "citation_rwr_iter10_dir       = \"gs://clpub/data_lake/arnet/tables/citation_rwr/iter-9\"\n",
    "citation_rwr_iter11_dir       = \"gs://clpub/data_lake/arnet/tables/citation_rwr/iter-10\"\n",
    "citation_rwr_iter12_dir       = \"gs://clpub/data_lake/arnet/tables/citation_rwr/iter-11\"\n",
    "\n",
    "from typing import Iterator, Tuple\n",
    "\n",
    "rwr_iter1 = spark.read.parquet(citation_rwr_iter1_dir)\n",
    "rwr_iter2 = spark.read.parquet(citation_rwr_iter2_dir)\n",
    "rwr_iter5 = spark.read.parquet(citation_rwr_iter5_dir)\n",
    "rwr_iter10 = spark.read.parquet(citation_rwr_iter10_dir)\n",
    "rwr_iter11 = spark.read.parquet(citation_rwr_iter11_dir)\n",
    "rwr_iter12 = spark.read.parquet(citation_rwr_iter12_dir)\n",
    "\n",
    "rwr_iter1.createOrReplaceTempView(\"rwr_iter1_df\")\n",
    "rwr_iter2.createOrReplaceTempView(\"rwr_iter2_df\")\n",
    "rwr_iter5.createOrReplaceTempView(\"rwr_iter5_df\")\n",
    "rwr_iter10.createOrReplaceTempView(\"rwr_iter10_df\")\n",
    "rwr_iter11.createOrReplaceTempView(\"rwr_iter11_df\")\n",
    "rwr_iter12.createOrReplaceTempView(\"rwr_iter12_df\")\n",
    "\n",
    "sample = rwr_iter1.sample(0.01).limit(50).repartition(1).withColumn(\n",
    "    \"datapoint\", sparkf.monotonically_increasing_id())\n",
    "sample.createOrReplaceTempView(\"rwr_sample\")\n",
    "\n",
    "sample_deviation = spark.sql(\"\"\"\n",
    "    select rs.datapoint, \n",
    "        rs.node._1 as rank1, \n",
    "        r2d.node._1 as rank2, \n",
    "        r5d.node._1 as rank5,\n",
    "        r10d.node._1 as rank10,\n",
    "        r11d.node._1 as rank11,\n",
    "        r12d.node._1 as rank12\n",
    "    from rwr_sample as rs\n",
    "        inner join rwr_iter2_df as r2d on rs.id = r2d.id\n",
    "        inner join rwr_iter5_df as r5d on rs.id = r5d.id\n",
    "        inner join rwr_iter10_df as r10d on rs.id = r10d.id\n",
    "        inner join rwr_iter11_df as r11d on rs.id = r11d.id\n",
    "        inner join rwr_iter12_df as r12d on rs.id = r12d.id\n",
    "\"\"\")\n",
    "\n",
    "sample_deviation.repartition(1).write.mode(\"overwrite\").csv(\"gs://clpub/diagram/rwr_distribution_changes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark.pyspark\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"datapoint\", LongType(), False),\n",
    "    StructField(\"rank1\", FloatType(), False),\n",
    "    StructField(\"rank2\", FloatType(), False),\n",
    "    StructField(\"rank5\", FloatType(), False),\n",
    "    StructField(\"rank10\", FloatType(), False),\n",
    "    StructField(\"rank11\", FloatType(), False),\n",
    "    StructField(\"rank12\", FloatType(), False),\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).csv(\"gs://clpub/diagram/rwr_distribution_changes\")\n",
    "df.createOrReplaceTempView(\"data\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
