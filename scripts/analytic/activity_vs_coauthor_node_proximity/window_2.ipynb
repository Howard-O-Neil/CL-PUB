{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "import pyspark.sql.functions as sparkf\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "coauthor_dir = \"gs://clpub/data_lake/arnet/tables/coauthor/merge-0\"\n",
    "author_activ_dir = \"gs://clpub/data_lake/arnet/tables/author_activity/merge-Wnd-2\"\n",
    "\n",
    "coauthor_schema = StructType([\n",
    "    StructField('_id', StringType(), False),\n",
    "    StructField('_status', IntegerType(), False),\n",
    "    StructField('_order', IntegerType(), False),\n",
    "    StructField('paper_id', StringType(), False),\n",
    "    StructField('paper_title', StringType(), False),\n",
    "    StructField('author1_id', StringType(), False),\n",
    "    StructField('author1_name', StringType(), False),\n",
    "    StructField('author1_org', StringType(), False),\n",
    "    StructField('author2_id', StringType(), False),\n",
    "    StructField('author2_name', StringType(), False),\n",
    "    StructField('author2_org', StringType(), False),\n",
    "    StructField('year', FloatType(), False),\n",
    "])\n",
    "\n",
    "author_activ_schema = StructType([\n",
    "    StructField('author_id', StringType(), False),\n",
    "    StructField('freq', FloatType(), False),\n",
    "])\n",
    "\n",
    "coauthor_df = spark.read.schema(coauthor_schema).parquet(coauthor_dir)\n",
    "\n",
    "author_activ_df = spark.read.schema(author_activ_schema).parquet(author_activ_dir)\n",
    "author_activ_df.createOrReplaceTempView(\"author_activ_df\")\n",
    "\n",
    "sample_coauthor = coauthor_df.sample(0.1, 999)\n",
    "sample_coauthor.createOrReplaceTempView(\"sample_coauth\")\n",
    "\n",
    "join_freq_df = spark.sql(\"\"\"\n",
    "    select sc.author1_id, sc.author2_id, aad1.freq as freq1, aad2.freq as freq2\n",
    "    from sample_coauth as sc\n",
    "        inner join author_activ_df as aad1 on aad1.author_id = sc.author1_id\n",
    "        inner join author_activ_df as aad2 on aad2.author_id = sc.author2_id\n",
    "    limit 5000\n",
    "\"\"\")\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import numpy as np\n",
    "\n",
    "def avg_freq_func(v1: pd.Series, v2: pd.Series) -> pd.Series:\n",
    "    v1_log = np.log(v1 + 10.)\n",
    "    v2_log = np.log(v2 + 10.)\n",
    "    return ((v1_log - v2_log).abs()) / (pd.concat([v1_log, v2_log], axis=1).max(axis=1))\n",
    "\n",
    "avg_freq = pandas_udf(avg_freq_func, returnType=FloatType())\n",
    "\n",
    "sample_avg = join_freq_df.repartition(1).withColumn(\"datapoint\", sparkf.monotonically_increasing_id()).select(\n",
    "    sparkf.col(\"datapoint\"), avg_freq(sparkf.col(\"freq1\"), sparkf.col(\"freq2\")).alias(\"node_proximity\"))\n",
    "\n",
    "sample_avg.repartition(1).write.mode(\"overwrite\").csv(\"gs://clpub/diagram/activity_vs_coauthor_node_proximity/window_2\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
